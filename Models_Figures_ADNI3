library(readxl)
library(sjPlot)
library(table1)


library(readxl)

library(dplyr)
library(readxl)

library(glmnet)

library(glmmLasso)

library(caret)

library(sjPlot)
library(devtools)
library(interactions)

library(egg)

library(yarrr)


data=read_excel("Master_DataBase_DL.xlsx")

look for partial correlation at the whole-sample level 

pcor(sample_data,method = c("kendall"))

sample_data <- subset(data,
                 select = c(PTEDUCAT,DMN_DAN,SUVR_temporalmetaro)

M = pcor(sample_data)
testRes = cor.mtest(sample_data, conf.level = 0.95)

corrplot(M$estimate, p.mat = testRes$p, method = 'color', diag = FALSE, type = 'upper',
          sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
          insig = 'label_sig', pch.col = 'grey20', order = 'AOE')

############ To  demonstrate that DMN DAN education level and ptau at the Temporal-Meta ROI are not correlated, and then: A) ptau is not a confounding variable 
in the correlation between DMN DAN and MOCA B) Education level cannot mediate the correlation between DMN DAN and MOCA

##### Look for group differences in DMN DAN anticorrelation in Fig 2 ########

install.packages("ggpubr")

library(ggpubr)

 my_comparisons1 <- list( c("A- C-","A+ C+"), c("A- C-","A+ C-"), c("A- C-","A+ C-"),c("A- C+","A+ C-"),c("A+ C-","A+ C+"),c("A+ C-","A+ C+"))

ggboxplot(data, x = "category_amyloid", y = "DMN_DAN",
          color = "category_amyloid", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons1) # Add pairwise comparisons p-value


my_comparisons2 <- list( c("A- C-","A+ C+"), c("A- C-","A+ C-"), c("A- C-","A+ C-"),c("A- C+","A+ C-"),c("A+ C-","A+ C+"),c("A+ C-","A+ C+"))

ggboxplot(data, x = "category_amyloid", y = "DMN_DAN",
          color = "category_amyloid", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons2) # Add pairwise comparisons p-value


then while patients were pooled aMCI + AD VS NC

p <- ggboxplot(data, x = "DiagnosesII", y = "DMN_DAN",
          color = "DiagnosesII", palette = "jco",
          add = "jitter")
#  Add p-value
p + stat_compare_means()
# Change method
p + stat_compare_means(method = "wilcox.test")

Run the regression linear models with k-folds cross-validation and boostrap in half-split sub-samples 

########## FIGURE 3 predictive models ################

### 67 % training : 33 % test data #### 

set.seed(1)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_control<-trainControl(method="repeatedcv", number=5, repeats=5, search=“random”)


model <- train(MOCA ~ PTEDUCAT+ PTGENDER+ DMN_DAN, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")


###### Store de regression models for the 25 Folds cross-validation #####  
 
fold_data <- lapply(model$control$index, function(index) train[index,]) %>% 
    bind_rows(.id = "Fold")

#### Plot them superimposed as in Figure 2 panel A) ##### 

plot1=ggplot(fold_data, aes(DMN_DAN,MOCA,col = Fold,fill=Fold),size=0.5, alpha=0.1) + 
     geom_point(shape = 21, fill = "red3",
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN DAN anti correlation", y = "MOCA") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_color_manual(values=c("Red","Purple","#006000","Brown","burlywood4","chocolate1","chartreuse4","blue4","darkorchid","darkmagenta","gray","khaki1","mediumorchid4","lightsteelblue1","navyblue","royalblue1","plum","sienna1","yellow3","blue4","tan1","violetred3","springgreen","yellow","burlywood4"))


## plot the boostrap distribution for every random-half partition ### 

install.packages("lmboot")
Seed=set.seed(11)
library (lmboot)

library(parallel)
library(readxl)
library(sjPlot)

library(boot)
library(lmboot)

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]
dev.new()
par(mfrow=c(1,2))
y=discovery[ ,c("MOCA")]
y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~  PTGENDER+PTEDUCAT +DMN_DAN, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig1=hist(WildObj$bootEstParam[,4],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

y=replication[ ,c("MOCA")]

y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~ PTGENDER+PTEDUCAT +DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig2=hist(WildObj$bootEstParam[,4],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)


##### To test wheter different random-split partitions affects the results ###########

set.seed(44)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_control<-trainControl(method="repeatedcv", number=5, repeats=5, search=“random”)

plot the boostrap distribution for every random-half partition 

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]
dev.new()
par(mfrow=c(1,2))
y=discovery[ ,c("MOCA")]
y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~ PTGENDER+PTEDUCAT +DMN_DAN, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig1=hist(WildObj$bootEstParam[,4],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

y=replication[ ,c("MOCA")]

y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~PTGENDER+PTEDUCAT +DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig2=hist(WildObj$bootEstParam[,4],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)


model <- train(MOCA ~PTEDUCAT+ PTGENDER+ DMN_DAN, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")


##### Having in account confounding factors such as PET tau at temporal meta ROIs ### 

set.seed(156);

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]
dev.new()
par(mfrow=c(1,2))
y=discovery[ ,c("MOCA")]
y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~ PTEDUCAT +DMN_DAN+SUVR_temporalmetaroi, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,3],)

fig1=hist(WildObj$bootEstParam[,3],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,3], probs=c(.025))
B=quantile(WildObj$bootEstParam[,3], probs=c(.975))
C=mean(WildObj$bootEstParam[,3],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,3], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

 y=replication[ ,c("MOCA")]

y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~Age+PTEDUCAT+DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,3],)

fig2=hist(WildObj$bootEstParam[,3],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,3], probs=c(.025))
B=quantile(WildObj$bootEstParam[,3], probs=c(.975))
C=mean(WildObj$bootEstParam[,3],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,3], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

#### PLOT the superimposed  regression lines for the  for models (5 folds repeated 10 times) the first 25 FOLDS for visualization####

 plot1=ggplot(fold_data, aes(DMN_DAN,MOCA,col = Fold,fill=Fold),size=0.5, alpha=0.1) + 
     geom_point(shape = 21, fill = "red3",
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN DAN anti correlation", y = "MOCA") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_color_manual(values=c("Red","Purple","#006000","Brown","burlywood4","chocolate1","chartreuse4","blue4","darkorchid","darkmagenta","gray","khaki1","mediumorchid4","lightsteelblue1","navyblue","royalblue1","plum","sienna1","yellow3","blue4","tan1","violetred3","springgreen","yellow","burlywood4"))

### Tunning hyperparamater with a grid to replicavility and test model performance ### 

#### This is the pannel B figure 3 ##### 

observed_C=test[ ,c("MOCA")]

observed_C <- as.numeric(unlist(observed_C))
mtry <- sqrt(ncol(train))

coco <- expand.grid(.mtyr=seq(0,0.1,by=0.01))
mdl = train(MOCA ~PTEDUCAT+ DMN_DAN,train,method="lm",
trControl=train_control, tunegrid=coco )

p_model<-predict(mdl,newdata=test)
r_model= cor.test (p_model,observed_C)

plot2=plot(p_model,observed_C, xlab="Observed MOCA" ,
         ylab="Predicted MOCA (repeated kflolds)",font.lab=1,cex.lab=1, cex.sub=1,pch = 19, frame = FALSE)

reg1<-lm(p_model~observed_C)

abline(reg1, col=“red”,lwd=5)

### Now with LASSO and ridge regression: look wheter the resutls stay ####

Not shown in figures but these results are in Table 2

train_y=train[ ,c("MOCA”)]

train_y <- as.numeric(unlist(train_y))

data$SUVR_temporalmetaroi=as.numeric(data$SUVR_temporalmetaroi)


train_x=model.matrix(MOCA ~Age +PTEDUCAT+ DMN_DAN+ SUVR_temporalmetaroi,train)[, -1] #Dropping the intercept column)

 ridge1_cv <- cv.glmnet(x = train_x, y =train_y,
                        ## type.measure: loss to use for cross-validation.
                        type.measure = "mse",
                        ## K = 10 is the default.
                        nfold = 10,
                        ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                        alpha = 0)
 plot(ridge1_cv)
 best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]

Age           Education  DMN_DAN      P Tau
-0.07762397  0.28092831 -1.82225008 -2.99078929

Age almos srink to 0...

#### AND LASSO cross-validated with repeated cv ###### 


train_control<-trainControl(method="repeatedcv", number=20, repeats=10)

mdl_lasso <- train(MOCA ~ Age+ PTEDUCAT+ DMN_DAN+ SUVR_temporalmetaroi,
   data = train,
   method = "glmnet",
   metric = "RMSE",
   preProcess = c("center", "scale"),
   tuneGrid = expand.grid(
     .alpha = 1,  # optimize a lasso regression
     .lambda = seq(0, 5, length.out = 101)
),
   trControl = train_control)

coef(mdl_lasso$finalModel, mdl_lasso$finalModel$lambdaOpt)
                             s1
(Intercept)          24.9834711
Age                  -0.6761096
PTEDUCAT              0.8046954
DMN_DAN              -0.5973258
SUVR_temporalmetaroi -1.9753868

### Is true that PET Tau measured as SUVR_temporalmetaroi explains most of the variability in cognition but DMN-DAN stay as a significant fMRI factor explaining cognitive decline in this cohort#####

###### Indeed Age shrink to as a factor and DMN DAN remains as an independent factor predicting MOCA here with 20 folds 10 repeated 10 times some researchers support 
that may lead to overfiiting (this is way I did it 10 folds 10 times bellow) ######

5 x 1 sparse Matrix of class "dgCMatrix"
                             s1
(Intercept)          25.0330579
Age                  -0.9437584
PTEDUCAT              1.1147966
DMN_DAN              -0.5783331
SUVR_temporalmetaroi -1.9397531

#### Now get pvalues for LASSO regression at the whole-sample (p-values based on the lasso projection method) Bonferroni corrected for multiple comparation ##### 

install.packages("hdi")
library(hdi)
 

x=model.matrix(MOCA ~ PTEDUCAT+ DMN_DAN+SUVR_temporalmetaro,data)[, -1] #Dropping the intercept column)


y=data[ ,c("MOCA")]

fit.ridge <- ridge.proj(x, y,multiplecorr.method = "bonferroni")
 

fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "bonferroni")
pvalues=fit.ridge.scaled$pval.corr
fit.lasso.robust <- lasso.proj(x, y, robust = TRUE)
pvalues_robust=fit.lasso.robust $pval.corr 

Resutls Robust LASSO while includign SUVR_temporalmetaroi

   PTEDUCAT             SUVR_temporalmetaroi         DMN_DAN 
        1.664993e-04         2.773450e-08         4.478622e-03 


####### Look at the coefficients while we do nested cross-validation of the LASSO models 10 times 10 repetitions ###########

set.seed(1)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_x=model.matrix(MOCA ~PTEDUCAT DMN_DAN+SUVR_temporalmetaroi,train)[, -1] #Dropping the intercept column)

train_control<-trainControl(method="repeatedcv", number=10, repeats=10)

mdl_lasso <- train(MOCA ~PTEDUCAT+ DMN_DAN+SUVR_temporalmetaroi,
   data = train,
   method = "glmnet",
   metric = "RMSE",
   preProcess = c("center", "scale"),
   tuneGrid = expand.grid(
     .alpha = 1,  # optimize a lasso regression
     .lambda = seq(0, 5, length.out = 101)
),
   trControl = train_control)

coef(mdl_lasso$finalModel, mdl_lasso$finalModel$lambdaOpt)

## Results LASSO  ### 

(Intercept)          24.9834711
PTEDUCAT              0.7615224
DMN_DAN              -0.7871511
SUVR_temporalmetaroi -1.8641643


##### Make predictions on test data and test model performance #### 

### Test prediction accuracy in the LASSO model to see if is consistent with k-fold #### 

pred.full <- predict(mdl_lasso,newdata=test)
pred.lasso <- as.vector(predict(mdl_lasso,test , s=mdl_lasso$finalModel$lambdaOpt))

Actual_MOCA= test[ ,c("MOCA")]
Actual_MOCA <- as.numeric(unlist(Actual_MOCA))

cor(Actual_MOCA,pred.lasso)

plot3=plot(Actual_MOCA,pred.lasso , xlab="Observed MOCA" ,
         ylab="Predicted MOCA (LASSO)",font.lab=1,cex.lab=1, cex.sub=1,pch = 19, frame = FALSE)

reg1<-lm(pred.lasso ~Actual_MOCA)


abline(reg1, col=“red”,lwd=5)


#######Check in the half sample coefs the last graph of FIG3 #####


## FIGURE 5 ### 

###### see whether in a set of half partitions the results replicate in both samples for the interactions ####


library(parallel)
library(readxl)
library(sjPlot)

library(boot)
library(lmboot)

set.seed(15);

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]


dev.new()


par(mfrow=c(1,2))


y=discovery[ ,c("MOCA")]


x1=model.matrix(MOCA ~ PTEDUCAT+ DMN_DAN+PTEDUCAT*DMN_DAN, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig1=hist(WildObj$bootEstParam[,4],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression) in discovery sample")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

y=replication[ ,c("MOCA")]

x1=model.matrix(MOCA ~ Age+ PTEDUCAT+ DMN_DAN+PTEDUCAT*DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig2=hist(WildObj$bootEstParam[,4],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression) in replication sample")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)


NOW look with gradient boosting machine in Figure 5 looking at feature importance


x=model.matrix(MOCA ~ PTEDUCAT+ DMN_DAN,data)[, -1] #Dropping the intercept column)

y=data[ ,c("MOCA")]

fit.ridge <- ridge.proj(x, y,multiplecorr.method = "bonferroni")
 

fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "bonferroni")
pvalues=fit.ridge.scaled$pval.corr
fit.lasso.robust <- lasso.proj(x, y, robust = TRUE)
pvalues_robust=fit.lasso.robust $pval.corr 


This gives a) the coefficients for the lasso regression models in Table 2 and b) the pvalues after Bonferroni correction

##### Make predictions on test data and test model performance #### 

Now look the accuracy of the model

pred.full <- predict(mdl_lasso,newdata=test)
pred.lasso <- as.vector(predict(mdl_lasso,test , s=mdl_lasso$finalModel$lambdaOpt))

Actual_MOCA= test[ ,c("MOCA")]
Actual_MOCA <- as.numeric(unlist(Actual_MOCA))

cor(Actual_MOCA,pred.lasso)

plot3=plot(Actual_MOCA,pred.lasso , xlab="Observed MOCA" ,
         ylab="Predicted MOCA (LASSO)",font.lab=1,cex.lab=1, cex.sub=1,pch = 19, frame = FALSE)

reg1<-lm(pred.lasso ~Actual_MOCA)


abline(reg1, col=“red”,lwd=5)


######## Fig 5 I used GBM because in half split partitions the results may change due to change of seed.. needs additional conformation with hyperparameter-free methods if the interaction education DMN ~ DAN is there or not ############ 
install.packages("gbm")
library (gbm)
model_gbm = gbm(MOCA ~ Age+PTEDUCAT+DMN_DAN+SUVR_temporalmetaroi,
 
                 data = data,
 
                 distribution = "gaussian",
 
                 cv.folds = 10,
 
                 shrinkage = .01,
 
                 n.minobsinnode = 10,
 
                 n.trees = 1000)
 
 summary (model_gbm)




##### TABLE1 #### 

This  table must be reproduced from the sketch by ADNI regulation metadata must be de-identified

library(readxl)
library(sjPlot)
library(table1)
library(gtsummary)

library(arsenal)
library(magrittr)

data=read_excel("Master_final_data_base_DLombardo.xlsx")

data$diagnoses=as.factor(data$diagnoses)

table1::label(data$diagnoses)  <- "Amyloid and cognitive status"

table1::label(data$Age)  <- "Age (years)"
table1::label(data$PTEDUCAT)  <- "Education (years)"
table1::label(data$PTGENDER)  <- "Sex"

table1::label(data$MOCA)  <- "MOCA score"

table1::label(data$MMSCORE)  <- "MM score"

table1::label(data$DMN_DAN)  <- "DMN DAN anticorrelation"

table1::label(data$average_FD)  <- "Framewise displacement (FD)"


table1 <- tableby(datat$category~ data$Age + data$PTEDUCAT+data$PTGENDER+data$MOCA+data$DMN_DAN+data$MMSCORE+data$average_FD, data=data)

write2word(table1,title= “Table 1”)


Supplementary FIGURE MOCA VS FD 
plot(data111$average_FD,data111$MOCA,main = "MOCA score vs mean head movement",xlab = "FD (in mm)",ylab = "MOCA score")

abline(lm(MOCA ~average_FD)   theme_minimal()




Suplementary Figures 

set.seed(1)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_control<-trainControl(method="repeatedcv", number=5, repeats=5, search=“random”)


model <- train(MOCA ~PTEDUCAT+ DMN_DAN+SUVR_temporalmetaroi, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")


Store de regression models for the 25 Folds cross-validation 
 
fold_data <- lapply(model$control$index, function(index) train[index,]) %>% 
    bind_rows(.id = "Fold")

Plot them superimposed as in Figure 2 panel A)

plot1=ggplot(fold_data, aes(DMN_DAN,MOCA,col = Fold,fill=Fold),size=0.5, alpha=0.1) + 
     geom_point(shape = 21, fill = "red3",
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN DAN anti correlation", y = "MOCA") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_color_manual(values=c("Red","Purple","#006000","Brown","burlywood4","chocolate1","chartreuse4","blue4","darkorchid","darkmagenta","gray","khaki1","mediumorchid4","lightsteelblue1","navyblue","royalblue1","plum","sienna1","yellow3","blue4","tan1","violetred3","springgreen","yellow","burlywood4"))

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)           24.9835     0.2529  98.784  < 2e-16 ***
PTEDUCAT               0.7704     0.2577   2.990  0.00341 ** 
DMN_DAN               -0.7959     0.2589  -3.074  0.00263 ** 
SUVR_temporalmetaroi  -1.8708     0.2622  -7.134 8.77e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.782 on 117 degrees of freedom
Multiple R-squared:  0.4312,	Adjusted R-squared:  0.4166 
F-statistic: 29.56 on 3 and 117 DF,  p-value: 2.669e-14

model <- train(MOCA ~ DMN_DAN, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")

When looking only to DMN DAN anticorrelation the model explains the 8% of the variance 

when looking at PET tau at the meta-temporal ROI explains 30% of the variance 

Knowing that while both are covariates the prediction of DMN_DAN on cognition remains significant which agrees with the regression LASSO 
coefficients for DMN DAN that do not shrink to zero when PET Tau was included in the 
 regression model

Get the tables for the distribution of variables in the half split partition for figure 2 and 3

set.seed(7);

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]

table1::label(discovery$Diagnoses)  <- "Amyloid and cognitive status"

table1::label(discovery$Age)  <- "Age (years)"
table1::label(discovery$PTEDUCAT)  <- "Education (years)"
table1::label(discovery$PTGENDER)  <- "Sex"

table1::label(discovery$MOCA)  <- "MOCA score"

table1::label(discovery$MOCA)  <- "MOCA score"

table1::label(discovery$MMSCORE)  <- "MM score"

table1::label(discovery$DMN_DAN)  <- "DMN DAN anticorrelation"

table1::label(discovery$SUVR_temporalmetaroi)  <- "PET SUVR Temporal meta ROi"

table1 <- tableby(discovery$Diagnoses~ discovery$Age + discovery$PTEDUCAT+ discovery$PTGENDER+ discovery$MOCA+discovery$DMN_DAN+discovery$MMSE+ discovery$SUVR_temporalmetaroi, data=discovery)
 write2word(table1,"~/trash3.doc",title= "Table 2")


table1::label(replication$Diagnoses)  <- "Amyloid and cognitive status"

table1::label(replication$Age)  <- "Age (years)"
table1::label(replication$PTEDUCAT)  <- "Education (years)"
table1::label(replication$PTGENDER)  <- "Sex"

table1::label(replication$MOCA)  <- "MOCA score"

table1::label(replication$MOCA)  <- "MOCA score"

table1::label(replication$MMSCORE)  <- "MM score"

table1::label(replication$DMN_DAN)  <- "DMN DAN anticorrelation"

table1::label(replication$SUVR_temporalmetaroi)  <- "PET SUVR Temporal meta ROi"

table2 <- tableby(replication$Diagnoses~ replication$Age + replication$PTEDUCAT+ replication$PTGENDER+ replication$MOCA+discovery$DMN_DAN+replication$MMSE+ replication$SUVR_temporalmetaroi, data=replication)
write2word(table,title= “Table 1”)
 write2word(table2,"~/trash4.doc",title= "Table 2")

