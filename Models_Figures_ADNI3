library(readxl)
library(sjPlot)
library(table1)


library(readxl)

library(dplyr)
library(readxl)

library(glmnet)

library(glmmLasso)

library(caret)

library(sjPlot)
library(devtools)
library(interactions)

library(egg)

library(yarrr)


data=read_excel("reviewed_database.xlsx")

look for partial correlation at the whole-sample level 

pcor(sample_data,method = c("kendall"))

sample_data <- subset(data,
                 select = c(PTEDUCAT,DMN_DAN,SUVR_temporalmetaro)

testRes = cor.mtest(sample_data, conf.level = 0.95)

corrplot(M$estimate, p.mat = testRes$p, method = 'color', diag = FALSE, type = 'upper',
          sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9,
          insig = 'label_sig', pch.col = 'grey20', order = 'AOE')

just for demonstrate that DMN DAN education level and ptau at Temporal Meta Roi are not correlated, and then: ptau is not a confounding variable 
in the correlation between DMN DAN and MOCA b) Education level cannot mediate the correlation between DMN DAN and MOCA

##### Look for group differences in DMN DAN anticorrelation in Fig 2 ########

install.packages("ggpubr")

library(ggpubr)

 my_comparisons1 <- list( c("A- C-","A+ C+"), c("A- C-","A+ C-"), c("A- C-","A+ C-"),c("A- C+","A+ C-"),c("A+ C-","A+ C+"),c("A+ C-","A+ C+"))

ggboxplot(data, x = "category_amyloid", y = "DMN_DAN",
          color = "category_amyloid", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons) # Add pairwise comparisons p-value


my_comparisons2 <- list( c("A- C-","A+ C+"), c("A- C-","A+ C-"), c("A- C-","A+ C-"),c("A- C+","A+ C-"),c("A+ C-","A+ C+"),c("A+ C-","A+ C+"))

ggboxplot(data, x = "category_amyloid", y = "DMN_DAN",
          color = "category_amyloid", palette = "jco")+ 
  stat_compare_means(comparisons = my_comparisons) # Add pairwise comparisons p-value


then while patients were pooled aMCI + AD VS NC

p <- ggboxplot(data, x = "DiagnosesII", y = "DMN_DAN",
          color = "DiagnosesII", palette = "jco",
          add = "jitter")
#  Add p-value
p + stat_compare_means()
# Change method
p + stat_compare_means(method = "wilcox.test")

Run the regression linear models with k-folds cross-validation and boostrap in half-split sub-samples 

########## FIGURE 3 predictive models ################


First 

67 % training : 33 % test data 

set.seed(7)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_control<-trainControl(method="repeatedcv", number=5, repeats=5, search=“random”)


model <- train(MOCA ~Age+PTEDUCAT+ PTGENDER+ DMN_DAN, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")


Store de regression models for the 25 Folds cross-validation 
 
fold_data <- lapply(model$control$index, function(index) train[index,]) %>% 
    bind_rows(.id = "Fold")

Plot them superimposed as in Figure 2 panel A)

plot1=ggplot(fold_data, aes(DMN_DAN,MOCA,col = Fold,fill=Fold),size=0.5, alpha=0.1) + 
     geom_point(shape = 21, fill = "red3",
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN DAN anti correlation", y = "MOCA") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_color_manual(values=c("Red","Purple","#006000","Brown","burlywood4","chocolate1","chartreuse4","blue4","darkorchid","darkmagenta","gray","khaki1","mediumorchid4","lightsteelblue1","navyblue","royalblue1","plum","sienna1","yellow3","blue4","tan1","violetred3","springgreen","yellow","burlywood4"))


plot the boostrap distribution for every random-half partition 

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]
dev.new()
par(mfrow=c(1,2))
y=discovery[ ,c("MOCA")]
y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~ Age+PTGENDER+PTEDUCAT +DMN_DAN, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,5],)

fig1=hist(WildObj$bootEstParam[,5],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,5], probs=c(.025))
B=quantile(WildObj$bootEstParam[,5], probs=c(.975))
C=mean(WildObj$bootEstParam[,5],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,5], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

y=replication[ ,c("MOCA")]

y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~Age+PTGENDER+PTEDUCAT +DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,5],)

fig2=hist(WildObj$bootEstParam[,5],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,5], probs=c(.025))
B=quantile(WildObj$bootEstParam[,5], probs=c(.975))
C=mean(WildObj$bootEstParam[,5],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,5], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)



To test wheter different random-split partitions affects the results: 

set.seed(77)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_control<-trainControl(method="repeatedcv", number=5, repeats=5, search=“random”)

plot the boostrap distribution for every random-half partition 

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]
dev.new()
par(mfrow=c(1,2))
y=discovery[ ,c("MOCA")]
y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~ Age+PTGENDER+PTEDUCAT +DMN_DAN, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,5],)

fig1=hist(WildObj$bootEstParam[,5],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,5], probs=c(.025))
B=quantile(WildObj$bootEstParam[,5], probs=c(.975))
C=mean(WildObj$bootEstParam[,5],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,5], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

y=replication[ ,c("MOCA")]

y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~Age+PTGENDER+PTEDUCAT +DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,5],)

fig2=hist(WildObj$bootEstParam[,5],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,5], probs=c(.025))
B=quantile(WildObj$bootEstParam[,5], probs=c(.975))
C=mean(WildObj$bootEstParam[,5],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,5], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)


model <- train(MOCA ~Age+PTEDUCAT+ PTGENDER+ DMN_DAN, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")


Having in account confounding factors such as PET tau at temporal meta ROIs

set.seed(156);

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]
dev.new()
par(mfrow=c(1,2))
y=discovery[ ,c("MOCA")]
y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~ PTEDUCAT +DMN_DAN+SUVR_temporalmetaroi, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,3],)

fig1=hist(WildObj$bootEstParam[,3],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,3], probs=c(.025))
B=quantile(WildObj$bootEstParam[,3], probs=c(.975))
C=mean(WildObj$bootEstParam[,3],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,3], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

 y=replication[ ,c("MOCA")]

y <- as.numeric(unlist(y))
x1=model.matrix(MOCA ~Age+PTEDUCAT+DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,3],)

fig2=hist(WildObj$bootEstParam[,3],main=expression(paste(" ")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,3], probs=c(.025))
B=quantile(WildObj$bootEstParam[,3], probs=c(.975))
C=mean(WildObj$bootEstParam[,3],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,3], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)


#### PLOT the superimposed  regression lines for the  for models (5 folds repeated 10 times) the first 25 FOLDS for visualization####

 plot1=ggplot(fold_data, aes(DMN_DAN,MOCA,col = Fold,fill=Fold),size=0.5, alpha=0.1) + 
     geom_point(shape = 21, fill = "red3",
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN DAN anti correlation", y = "MOCA") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_color_manual(values=c("Red","Purple","#006000","Brown","burlywood4","chocolate1","chartreuse4","blue4","darkorchid","darkmagenta","gray","khaki1","mediumorchid4","lightsteelblue1","navyblue","royalblue1","plum","sienna1","yellow3","blue4","tan1","violetred3","springgreen","yellow","burlywood4"))


### Tunning hyperparamater with a grid to replicavility and test model performance ### 

This is the pannel B figure 3

observed_C=test[ ,c("MOCA")]

observed_C <- as.numeric(unlist(observed_C))
mtry <- sqrt(ncol(train))

coco <- expand.grid(.mtyr=seq(0,0.1,by=0.01))
mdl = train(MOCA ~Age +PTEDUCAT+ DMN_DAN,train,method="lm",
trControl=train_control, tunegrid=coco )

p_model<-predict(mdl,newdata=test)
r_model= cor.test (p_model,observed_C)


plot2=plot(p_model,observed_C, xlab="Observed MOCA" ,
         ylab="Predicted MOCA (repeated kflolds)",font.lab=1,cex.lab=1, cex.sub=1,pch = 19, frame = FALSE)

reg1<-lm(p_model~observed_C)

abline(reg1, col=“red”,lwd=5)


### Now we go with LASSO ####

Not shown in figures but these results are in Table 2

x=model.matrix(MOCA ~ PTEDUCAT+ DMN_DAN+SUVR_temporalmetaro,data)[, -1] #Dropping the intercept column)


y=data[ ,c("MOCA")]

fit.ridge <- ridge.proj(x, y,multiplecorr.method = "bonferroni")
 

fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "bonferroni")
pvalues=fit.ridge.scaled$pval.corr
fit.lasso.robust <- lasso.proj(x, y, robust = TRUE)
pvalues_robust=fit.lasso.robust $pval.corr 

Resutls Robust LASSO while includign SUVR_temporalmetaroi

   PTEDUCAT             SUVR_temporalmetaroi         DMN_DAN 
        1.664993e-04         2.773450e-08         4.478622e-03 


Look at the coeffieents while we do nested cross-validation of the LASSO models

set.seed(1)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]


train_x=model.matrix(MOCA ~PTEDUCAT DMN_DAN+SUVR_temporalmetaroi,train)[, -1] #Dropping the intercept column)

 train_control<-trainControl(method="repeatedcv", number=10, repeats=10)

mdl_lasso <- train(MOCA ~PTEDUCAT+ DMN_DAN+SUVR_temporalmetaroi,
   data = train,
   method = "glmnet",
   metric = "RMSE",
   preProcess = c("center", "scale"),
   tuneGrid = expand.grid(
     .alpha = 1,  # optimize a lasso regression
     .lambda = seq(0, 5, length.out = 101)
),
   trControl = train_control)

coef(mdl_lasso$finalModel, mdl_lasso$finalModel$lambdaOpt)

Results LASSO 

(Intercept)          24.9834711
PTEDUCAT              0.7615224
DMN_DAN              -0.7871511
SUVR_temporalmetaroi -1.8641643


##### Make predictions on test data and test model performance #### 

Test prediction accuracy in the LASSO model to see if is consistent with k-folds 

pred.full <- predict(mdl_lasso,newdata=test)
pred.lasso <- as.vector(predict(mdl_lasso,test , s=mdl_lasso$finalModel$lambdaOpt))

Actual_MOCA= test[ ,c("MOCA")]
Actual_MOCA <- as.numeric(unlist(Actual_MOCA))

cor(Actual_MOCA,pred.lasso)

plot3=plot(Actual_MOCA,pred.lasso , xlab="Observed MOCA" ,
         ylab="Predicted MOCA (LASSO)",font.lab=1,cex.lab=1, cex.sub=1,pch = 19, frame = FALSE)

reg1<-lm(pred.lasso ~Actual_MOCA)


abline(reg1, col=“red”,lwd=5)


#######Check in the half sample coefs the last graph of FIG3 #####


FIGURE 5

###### see whether in a set of half partitions the results replicate in both samples for the interactions ####


library(parallel)
library(readxl)
library(sjPlot)

library(boot)
library(lmboot)

set.seed(15);

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]


dev.new()


par(mfrow=c(1,2))


y=discovery[ ,c("MOCA")]


x1=model.matrix(MOCA ~ PTEDUCAT+ DMN_DAN+PTEDUCAT*DMN_DAN, discovery)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig1=hist(WildObj$bootEstParam[,4],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression) in discovery sample")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lty=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)

y=replication[ ,c("MOCA")]

x1=model.matrix(MOCA ~ Age+ PTEDUCAT+ DMN_DAN+PTEDUCAT*DMN_DAN,replication)[, -1] #Dropping the intercept column)

WildObj <- wild.boot(y ~ x1, B=60000, seed=Seed)
WildObj$origEstParam
mean(WildObj$bootEstParam[,4],)

fig2=hist(WildObj$bootEstParam[,4],main=expression(paste(" boostraped samples of ",beta,~ "coeficients (linear regression) in replication sample")),
xlab= " Betas DMN DAN connectivity", ylab= "Count (in 60000 Boostrap Replications)",col=c("lightblue"),cex.axis = 1.5, cex.lab = 1.2)

A=quantile(WildObj$bootEstParam[,4], probs=c(.025))
B=quantile(WildObj$bootEstParam[,4], probs=c(.975))
C=mean(WildObj$bootEstParam[,4],)
abline(v = A, col="black", lwd=2, lay=1)
abline(v = B, col="blue", lwd=2, lty=1)
abline(v = C, col="red", lwd=3, lty=2)

rug(WildObj$bootEstParam[,4], ticksize = 0.03, side = 1, lwd = 0.1, col = "purple",
    quiet = getOption("warn") < 0)


NOW look with gradient boosting machine in Figure 5 looking at feature importance


x=model.matrix(MOCA ~ PTEDUCAT+ DMN_DAN,data)[, -1] #Dropping the intercept column)

y=data[ ,c("MOCA")]

fit.ridge <- ridge.proj(x, y,multiplecorr.method = "bonferroni")
 

fit.ridge.scaled  <- ridge.proj(x, y, betainit = "scaled lasso",multiplecorr.method = "bonferroni")
pvalues=fit.ridge.scaled$pval.corr
fit.lasso.robust <- lasso.proj(x, y, robust = TRUE)
pvalues_robust=fit.lasso.robust $pval.corr 


This gives a) the coefficients for the lasso regression models in Table 2 and b) the pvalues after Bonferroni correction

##### Make predictions on test data and test model performance #### 

Now look the accuracy of the model

pred.full <- predict(mdl_lasso,newdata=test)
pred.lasso <- as.vector(predict(mdl_lasso,test , s=mdl_lasso$finalModel$lambdaOpt))

Actual_MOCA= test[ ,c("MOCA")]
Actual_MOCA <- as.numeric(unlist(Actual_MOCA))

cor(Actual_MOCA,pred.lasso)

plot3=plot(Actual_MOCA,pred.lasso , xlab="Observed MOCA" ,
         ylab="Predicted MOCA (LASSO)",font.lab=1,cex.lab=1, cex.sub=1,pch = 19, frame = FALSE)

reg1<-lm(pred.lasso ~Actual_MOCA)


abline(reg1, col=“red”,lwd=5)


######## Fig 5 I used GBM because in half split partitions the results may change due to change of seed.. needs additional conformation with hyperparameter-free methods if the interaction education DMN ~ DAN is there or not ############ 

library(gbm)

model_gbmDMN_DAN = gbm(ge +PTEDUCAT+ PTGENDER+ DMN_DAN+DMN_DAN*PTEDUCAT,,

                data = train,

                distribution = "gaussian",

                cv.folds = 10,

                shrinkage = .01,

                n.minobsinnode = 10,

                n.trees = 1000)

summary (model_gbmDMN_DAN)



##### TABLE1 #### 

This  table must be reproduced from the sketch by ADNI regulation metadata must be de-identified

library(readxl)
library(sjPlot)
library(table1)
library(gtsummary)

library(arsenal)
library(magrittr)

data=read_excel("Master_final_data_base_DLombardo.xlsx")

data$diagnoses=as.factor(data$diagnoses)

table1::label(data$diagnoses)  <- "Amyloid and cognitive status"

table1::label(data$Age)  <- "Age (years)"
table1::label(data$PTEDUCAT)  <- "Education (years)"
table1::label(data$PTGENDER)  <- "Sex"

table1::label(data$MOCA)  <- "MOCA score"

table1::label(data$MMSCORE)  <- "MM score"

table1::label(data$DMN_DAN)  <- "DMN DAN anticorrelation"

table1::label(data$average_FD)  <- "Framewise displacement (FD)"


table1 <- tableby(datat$category~ data$Age + data$PTEDUCAT+data$PTGENDER+data$MOCA+data$DMN_DAN+data$MMSCORE+data$average_FD, data=data)

write2word(table1,title= “Table 1”)


Supplementary FIGURE MOCA VS FD 
plot(data111$average_FD,data111$MOCA,main = "MOCA score vs mean head movement",xlab = "FD (in mm)",ylab = "MOCA score")

abline(lm(MOCA ~average_FD)   theme_minimal()




Suplementary Figures 

set.seed(1)

half_size <- floor(0.67* nrow(data)) 

random_sample <- sample(seq_len(nrow(data)), size = half_size)

train <- data[random_sample,]

test <- data[-random_sample,]

train_control<-trainControl(method="repeatedcv", number=5, repeats=5, search=“random”)


model <- train(MOCA ~PTEDUCAT+ DMN_DAN+SUVR_temporalmetaroi, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")


Store de regression models for the 25 Folds cross-validation 
 
fold_data <- lapply(model$control$index, function(index) train[index,]) %>% 
    bind_rows(.id = "Fold")

Plot them superimposed as in Figure 2 panel A)

plot1=ggplot(fold_data, aes(DMN_DAN,MOCA,col = Fold,fill=Fold),size=0.5, alpha=0.1) + 
     geom_point(shape = 21, fill = "red3",
               color = "red2", size = 3) +  theme_classic() + labs(x =  "DMN DAN anti correlation", y = "MOCA") + theme(axis.title = element_text(size = 15))+theme(axis.line = element_line(linewidth = 1, colour = "black"))+ theme(axis.text.y = element_text(size = 15))+ theme(axis.text.x = element_text(size = 15))+
      geom_smooth(method = "lm", formula="y~x", alpha=0.01,show.legend = FALSE)  + scale_color_manual(values=c("Red","Purple","#006000","Brown","burlywood4","chocolate1","chartreuse4","blue4","darkorchid","darkmagenta","gray","khaki1","mediumorchid4","lightsteelblue1","navyblue","royalblue1","plum","sienna1","yellow3","blue4","tan1","violetred3","springgreen","yellow","burlywood4"))

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)           24.9835     0.2529  98.784  < 2e-16 ***
PTEDUCAT               0.7704     0.2577   2.990  0.00341 ** 
DMN_DAN               -0.7959     0.2589  -3.074  0.00263 ** 
SUVR_temporalmetaroi  -1.8708     0.2622  -7.134 8.77e-11 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.782 on 117 degrees of freedom
Multiple R-squared:  0.4312,	Adjusted R-squared:  0.4166 
F-statistic: 29.56 on 3 and 117 DF,  p-value: 2.669e-14

model <- train(MOCA ~ DMN_DAN, data =train, selectionFunction = "best",method = "lm", trControl = train_control, preProcess = c("center", "scale"),savePredictions= "final")

When looking only to DMN DAN anticorrelation the model explains the 8% of the variance 

when looking at PET tau at the meta-temporal ROI explains 30% of the variance 

Knowing that while both are covariates the prediction of DMN_DAN on cognition remains significant which agrees with the regression LASSO 
coefficients for DMN DAN that do not shrink to zero when PET Tau was included in the 
 regression model

Get the tables for the distribution of variables in the half split partition for figure 2 and 3

set.seed(7);

half_size <- floor(0.50 * nrow(data))

random_sample <- sample(seq_len(nrow(data)), size = half_size)

discovery <- data[random_sample, ]

replication <- data[-random_sample, ]

table1::label(discovery$Diagnoses)  <- "Amyloid and cognitive status"

table1::label(discovery$Age)  <- "Age (years)"
table1::label(discovery$PTEDUCAT)  <- "Education (years)"
table1::label(discovery$PTGENDER)  <- "Sex"

table1::label(discovery$MOCA)  <- "MOCA score"

table1::label(discovery$MOCA)  <- "MOCA score"

table1::label(discovery$MMSCORE)  <- "MM score"

table1::label(discovery$DMN_DAN)  <- "DMN DAN anticorrelation"

table1::label(discovery$SUVR_temporalmetaroi)  <- "PET SUVR Temporal meta ROi"

table1 <- tableby(discovery$Diagnoses~ discovery$Age + discovery$PTEDUCAT+ discovery$PTGENDER+ discovery$MOCA+discovery$DMN_DAN+discovery$MMSE+ discovery$SUVR_temporalmetaroi, data=discovery)
 write2word(table1,"~/trash3.doc",title= "Table 2")


table1::label(replication$Diagnoses)  <- "Amyloid and cognitive status"

table1::label(replication$Age)  <- "Age (years)"
table1::label(replication$PTEDUCAT)  <- "Education (years)"
table1::label(replication$PTGENDER)  <- "Sex"

table1::label(replication$MOCA)  <- "MOCA score"

table1::label(replication$MOCA)  <- "MOCA score"

table1::label(replication$MMSCORE)  <- "MM score"

table1::label(replication$DMN_DAN)  <- "DMN DAN anticorrelation"

table1::label(replication$SUVR_temporalmetaroi)  <- "PET SUVR Temporal meta ROi"

table2 <- tableby(replication$Diagnoses~ replication$Age + replication$PTEDUCAT+ replication$PTGENDER+ replication$MOCA+discovery$DMN_DAN+replication$MMSE+ replication$SUVR_temporalmetaroi, data=replication)
write2word(table,title= “Table 1”)
 write2word(table2,"~/trash4.doc",title= "Table 2")

